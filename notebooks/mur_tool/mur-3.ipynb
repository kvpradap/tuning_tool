{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2017-09-17T18:19:28.279191Z",
     "start_time": "2017-09-17T13:19:28.274708-05:00"
    },
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import string\n",
    "import math\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2017-09-17T18:13:54.709883Z",
     "start_time": "2017-09-17T13:13:54.572766-05:00"
    },
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import dmagellan\n",
    "from dmagellan.utils.py_utils.utils import build_inv_index, tokenize_strings\n",
    "from dmagellan.tokenizer.whitespacetokenizer import WhiteSpaceTokenizer\n",
    "from dmagellan.utils.cy_utils.stringcontainer import StringContainer\n",
    "from dmagellan.blocker.overlap.overlapblocker import OverlapBlocker"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2017-09-17T18:13:46.263827Z",
     "start_time": "2017-09-17T13:13:46.254581-05:00"
    },
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# functions\n",
    "def remove_stopwords(tokens, stopwords):\n",
    "    out_tokens = []\n",
    "    for token in tokens:\n",
    "        if not stopwords.has_key(token):\n",
    "            out_tokens.append(token)\n",
    "    return out_tokens\n",
    "def process_column(column, stop_words):\n",
    "    column = column.str.translate(None, string.punctuation)\n",
    "    column = column.str.lower()\n",
    "    if stop_words:\n",
    "        dict_stopwords = dict(zip(self.stop_words, [0] * len(self.stop_words)))\n",
    "        partial_rm_stopwords_fn = partial(remove_stopwords,\n",
    "                                          stopwords=dict_stopwords)\n",
    "        column = column.str.split().map(partial_rm_stopwords_fn).str.join(' ')\n",
    "    return column\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2017-09-17T18:17:21.237107Z",
     "start_time": "2017-09-17T13:17:19.314263-05:00"
    },
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "ltable = pd.read_csv('../../datasets/sample_citeseer_100k.csv')\n",
    "rtable = pd.read_csv('../../datasets/sample_dblp_100k.csv')\n",
    "\n",
    "lid, rid = 'id', 'id'\n",
    "l_block_attr, r_block_attr = 'title', 'title'\n",
    "tok = WhiteSpaceTokenizer()\n",
    "\n",
    "nbins = 10\n",
    "sample_proportion = 0.1\n",
    "seed = 0\n",
    "stopwords=[]\n",
    "\n",
    "\n",
    "ob = OverlapBlocker()\n",
    "p = ob.process_and_tokenize_ltable(ltable, 'id', 'title', tok, [])\n",
    "inv_index = build_inv_index([p])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "sample ltable"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2017-09-17T18:18:45.864593Z",
     "start_time": "2017-09-17T13:18:45.606182-05:00"
    }
   },
   "outputs": [],
   "source": [
    "    ltbl = ltable[[lid, l_block_attr]]\n",
    "    ltbl = ltbl[~ltbl[l_block_attr].isnull()]\n",
    "    #process columns\n",
    "    ltbl[l_block_attr] = process_column(ltbl[l_block_attr], stopwords)\n",
    "    n = int(math.floor(sample_proportion*len(ltable)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2017-09-17T18:19:04.580772Z",
     "start_time": "2017-09-17T13:19:04.459080-05:00"
    }
   },
   "outputs": [],
   "source": [
    "    # get the string lengths\n",
    "    ltbl['str_len'] = ltbl.title.str.len()\n",
    "    groups = ltbl.groupby('str_len')\n",
    "\n",
    "    len_ids = {}\n",
    "    for gid, g in groups:\n",
    "        len_ids[gid] = list(g[lid].values)\n",
    "    strlens = list(ltbl['str_len'].values)\n",
    "    strlens += [max(strlens) + 1]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2017-09-17T18:22:03.202496Z",
     "start_time": "2017-09-17T13:22:03.176392-05:00"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "261"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "    # get boundaries for equal width histogram\n",
    "    freq, edges = np.histogram(strlens, bins=nbins)\n",
    "\n",
    "    bins = [[] for _ in range(nbins)]\n",
    "    keys = sorted(len_ids.keys())\n",
    "    # find the bins where the keys should land.\n",
    "    positions = np.digitize(keys, edges)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2017-09-17T18:23:38.791473Z",
     "start_time": "2017-09-17T13:23:38.663260-05:00"
    }
   },
   "outputs": [],
   "source": [
    "# populate the ids of ltable in their corresponding bins\n",
    "for i in range(len(keys)):\n",
    "    k, p = keys[i], positions[i]\n",
    "    bins[p - 1].extend(len_ids[k])\n",
    "len_bins = [len(bins[i]) for i in range(len(bins))]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2017-09-17T18:31:22.468589Z",
     "start_time": "2017-09-17T13:31:22.463092-05:00"
    }
   },
   "outputs": [],
   "source": [
    "    # Compute the weight of each bin, based on the number of tuples in that bin and the total number of tuples\n",
    "    weights = [len_bins[i] / float(sum(len_bins)) for i in range(len(bins))]\n",
    "    # Based on the weights, find the number of tuples to be sampled from each bin\n",
    "    numtups = [int(math.ceil(weights[i] * n)) for i in range(len(weights))]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2017-09-17T18:31:23.629330Z",
     "start_time": "2017-09-17T13:31:23.612333-05:00"
    },
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "    # Based on the num. of tuples to be sampled from each bin, take a random sample\n",
    "    sampled = []\n",
    "    for i in range(len(bins)):\n",
    "        nt = numtups[i]\n",
    "        np.random.seed(0)\n",
    "        if len(bins[i]):\n",
    "            np.random.seed(seed)\n",
    "            tmp = np.random.choice(bins[i], nt)\n",
    "            if len(tmp):\n",
    "                sampled.extend(tmp)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2017-09-17T18:31:25.601088Z",
     "start_time": "2017-09-17T13:31:25.500613-05:00"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(        id                                              title  \\\n",
       " 0  1008177                       Rank-Tolerance Graph Classes   \n",
       " 1   435899  A Versatile Incompressible Navier Stokes Solve...   \n",
       " 2  1732361  MBT04 Preliminary Version Lightweight Specific...   \n",
       " 3   264023  Exploiting Model Uncertainty Estimates for Saf...   \n",
       " 4   173582  Robust Network Connectivity: when its the big ...   \n",
       " \n",
       "                                              authors journal  month    year  \\\n",
       " 0        Martin Charles, Golumbic Robert, E. Jamison     NaN    NaN  2003.0   \n",
       " 1  Marc Garbey, Francois Pacull, Keywords Navier ...     NaN    NaN  2006.0   \n",
       " 2                         Seung Mo Cho, Jae Wook Lee     NaN    NaN     NaN   \n",
       " 3                                    Je G. Schneider     NaN    NaN     NaN   \n",
       " 4                      Enoch Peserico, Larry Rudolph     NaN    NaN     NaN   \n",
       " \n",
       "   publication_type  \n",
       " 0              NaN  \n",
       " 1              NaN  \n",
       " 2              NaN  \n",
       " 3              NaN  \n",
       " 4              NaN  , 10004)"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ltable.set_index(lid, inplace=True, drop=False)\n",
    "ltable['_pos'] = list(range(len(ltable)))\n",
    "s_ltable = ltable.loc[sampled]\n",
    "s_ltable = s_ltable.sort_values(['_pos'])\n",
    "s_ltable.reset_index(drop=True, inplace=True)\n",
    "s_ltable.drop(['_pos'], axis=1, inplace=True)\n",
    "s_ltable.head(), len(s_ltable)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "sample rtable"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2017-09-17T18:40:39.183442Z",
     "start_time": "2017-09-17T13:40:39.179469-05:00"
    },
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "nbins = 10"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2017-09-17T18:40:40.383989Z",
     "start_time": "2017-09-17T13:40:39.185672-05:00"
    }
   },
   "outputs": [],
   "source": [
    "    ob = OverlapBlocker()\n",
    "    tok = WhiteSpaceTokenizer()\n",
    "    # rtbl = rtable.reset_index(drop=True)\n",
    "    rtbl = rtable[[rid, r_block_attr]]\n",
    "    # rtbl['_pos'] = list(range(len(tbl)))\n",
    "    p = ob.process_and_tokenize_ltable(rtbl, rid, r_block_attr, tok, stopwords)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2017-09-17T18:40:41.086881Z",
     "start_time": "2017-09-17T13:40:40.386375-05:00"
    }
   },
   "outputs": [],
   "source": [
    "    tok_cnt = {}\n",
    "    tok_map = {}\n",
    "    for i in range(p.size()):\n",
    "        tid, tokens = p.get(i)\n",
    "        cnt = 0\n",
    "        for tok in tokens:\n",
    "            if tok not in tok_map:\n",
    "                tok_map[tok] = len(inv_index.values(tok))\n",
    "            cnt += tok_map[tok]\n",
    "        tok_cnt[tid] = cnt\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2017-09-17T18:40:49.029704Z",
     "start_time": "2017-09-17T13:40:41.089141-05:00"
    }
   },
   "outputs": [],
   "source": [
    "    df =  pd.DataFrame(tok_cnt.items(), columns=['id', 'count'])\n",
    "    groups = df.groupby('count')\n",
    "    cnt_ids = {}\n",
    "    for gid, g in groups:\n",
    "        cnt_ids[gid] = list(g[lid].values)\n",
    "    cnts = list(df['count'].values)\n",
    "    cnts += [max(cnts) + 1]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2017-09-17T18:40:49.072810Z",
     "start_time": "2017-09-17T13:40:49.032183-05:00"
    },
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "    freq, edges = np.histogram(cnts, bins=nbins)\n",
    "    n = int(math.floor(sample_proportion * len(rtable)))\n",
    "    bins = [[] for _ in range(nbins)]\n",
    "    keys = sorted(cnt_ids.keys())\n",
    "    positions = np.digitize(keys, edges)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2017-09-17T18:40:49.294065Z",
     "start_time": "2017-09-17T13:40:49.074890-05:00"
    },
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "    for i in range(len(keys)):\n",
    "        k, p = keys[i], positions[i]\n",
    "        bins[p - 1].extend(cnt_ids[k])\n",
    "    len_bins = [len(bins[i]) for i in range(len(bins))]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2017-09-17T18:40:49.302621Z",
     "start_time": "2017-09-17T13:40:49.296551-05:00"
    },
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "    weights = [len_bins[i] / float(sum(len_bins)) for i in range(len(bins))]\n",
    "    numtups = [int(math.ceil(weights[i] * n)) for i in range(len(weights))]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2017-09-17T18:40:49.326696Z",
     "start_time": "2017-09-17T13:40:49.305637-05:00"
    },
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "    sampled = []\n",
    "    for i in range(len(bins)):\n",
    "        nt = numtups[i]\n",
    "        np.random.seed(seed)\n",
    "        if len(bins[i]):\n",
    "            tmp = np.random.choice(bins[i], nt)\n",
    "            if len(tmp):\n",
    "                sampled.extend(tmp)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2017-09-17T18:42:26.491249Z",
     "start_time": "2017-09-17T13:42:26.387207-05:00"
    },
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "    rtable['_pos'] = list(range(len(rtable)))\n",
    "    rtable.set_index(rid, inplace=True, drop=False)\n",
    "    s_rtable = rtable.loc[sampled]\n",
    "    s_rtable = s_rtable.sort_values('_pos')\n",
    "    s_rtable.drop(['_pos'], axis=1, inplace=True)\n",
    "    rtable.drop(['_pos'], axis=1, inplace=True)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
